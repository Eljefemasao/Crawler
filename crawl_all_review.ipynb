{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Amazon]対象のカテゴリの商品URLを全てCrawlingしtext形式でローカルに保存する\n",
    "./text/all_goods_urls.txtに保存されます。<br>\n",
    "実行するには以下スクリプト中のグローバル変数CATEGORYに特定カテゴリ(オムツなど)の階層のurlを入力して実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.parse import urljoin\n",
    "import sys\n",
    "from get_category_url import fetch\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "\n",
    "CATEGORY = '' # クローリング対象カテゴリurl\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "5 メリーズ全69件の商品のpageをcrawling  crawl_all_merise_goods.py    ◀◀◀◀◀　今ここ\n",
    "\n",
    "6 全69件に対応するすべてのカスタマーレビューを取得     crawl_all_review.py   \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    base_url = 'https://www.amazon.co.jp/'\n",
    "    category = CATEGORY\n",
    "    html = fetch(category)\n",
    "    all_urls = get_all_goods_review_pages_url(html, base_url)\n",
    "\n",
    "\n",
    "def scrape(html):\n",
    "    \"\"\"\n",
    "    引数htmlで与えられたhtml(おむつpage)からメリーズ全69件の商品のページ(html形式)およびURLを取得\n",
    "    :param html:　メリーズ商品ページをhtml形式で保持\n",
    "    :return: str型でメリーズ全68件の商品のページ(html形式)およびURL\n",
    "    \"\"\"\n",
    "    url = []\n",
    "    doc = soup(html, \"html.parser\")\n",
    "    a_tabs = doc.findAll(\"a\", class_=\"a-link-normal a-text-normal\")\n",
    "    for a_tab in a_tabs:\n",
    "        if hasattr(a_tab, \"attrs\"):\n",
    "            # 商品のパスを取得\n",
    "            goods_path = a_tab.attrs['href']\n",
    "            # 商品のURLに重複があれば消す\n",
    "            for i in range(len(url)):\n",
    "                if goods_path == url[i]:\n",
    "                    del url[i]\n",
    "            # 商品のパスが絶対パスのもののみ取得(商品ヘルプページを取り除く)  絶対パスの場合=False\n",
    "            if os.path.isabs(goods_path):\n",
    "                pass\n",
    "            else:\n",
    "                url.append(goods_path)\n",
    "    return url\n",
    "\n",
    "\n",
    "def scrape_next_button(html, base_url):\n",
    "\n",
    "    doc = soup(html, \"html.parser\")\n",
    "    div = doc.find(\"div\", id=\"pagn\")\n",
    "\n",
    "    if div is not None:\n",
    "        # \"次へ\"ボタンのa_tabを取得\n",
    "        next_a_tab = 0\n",
    "        for a in div.find_all(\"a\"):\n",
    "            next_a_tab = a\n",
    "        abs_path = urljoin(base_url, quote(next_a_tab.attrs['href'].encode(\"utf-8\")))\n",
    "        return abs_path, div\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_all_goods_review_pages_url(html, base_rul):\n",
    "\n",
    "    all_url = []\n",
    "    \n",
    "    while True:\n",
    "        time.sleep(3)\n",
    "        abs_path, div = scrape_next_button(html, base_rul)\n",
    "        if abs_path is not None and div is not None:\n",
    "            html = fetch(abs_path)\n",
    "            urls = scrape(html)\n",
    "            all_url.append(urls)\n",
    "            for url in urls:\n",
    "                print(url)\n",
    "            with open(\"./text/all_goods_urls.txt\",\"a\") as f:\n",
    "                for url in urls:\n",
    "                    f.write(url+\"\\n\")\n",
    "            if div.find_all(\"li\", class_=\"a-disabled a-last\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return all_url\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Amazon]text形式で保存された各商品URLを順次読み込み、各商品ごとに全てのreviewをcrawlingする\n",
    "./text/all_goods_urls.textより順次urlを読み込み./reviews/カテゴリ/商品ID/高評価・低評価/reviewページ/　のように保存されます。\n",
    "グローバル変数GOODSNAMEに商品カテゴリ(body_careなど)を入力し実行してください。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import quote\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "\n",
    "BASEURL = 'https://www.amazon.co.jp/' # アマゾンホームページ\n",
    "GOODSNAME = \" \"  # クローリングする商品名(ローカルに保存するディレクトリ名)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "6 ファイルに保存された全商品に対応するすべてのカスタマーレビューをクローリング      crawl_all_review.py  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fetch(url):\n",
    "    \"\"\"\n",
    "    引数urlで与えられたURLのwebページをhtml形式で取得する\n",
    "    WebページのエンコーディングはContent-Typeヘッダーより取得する\n",
    "    :param url:　amazonのホームページurl\n",
    "    :return: str型のamazonのホームページのカテゴリーページURL\n",
    "    \"\"\"\n",
    "    # resolve socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
    "    session = requests.session()\n",
    "    session.proxies = {'http': 'socks5h://127.0.0.1:9150', 'https': 'socks5h://127.0.0.1:9150'}\n",
    "\n",
    "    # urlがbyte文字列の場合utf-8にdecodeしておく\n",
    "    if type(url) == bytes:\n",
    "        url = url.decode(\"utf-8\")\n",
    "\n",
    "    # 複数回のクローリングを実行するとアマゾンのサーバーから拒絶されたためUser-Agentヘッダーの値を偽造\n",
    "    UA = UserAgent()\n",
    "    ua = UA.safari\n",
    "\n",
    "    # スマホページからのアクセスをカット\n",
    "    if \"(iPad;\" in ua:\n",
    "        # uaをアップデート\n",
    "        ua = UA.safari\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    headers = {'User-Agent': ua}\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "\n",
    "    # ステータスコードを確認しresponseが成功しているか確認する\n",
    "    if not r.status_code == 200:\n",
    "        print(\"response.status_code is not 200\")\n",
    "        return None\n",
    "\n",
    "    r.encoding = r.apparent_encoding\n",
    "\n",
    "    \"\"\"\n",
    "    request = urllib.request.Request(url=base_url, headers=headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "\n",
    "    # Httpヘッダーよりエンコーディングを取得する(明示されていない場合はutf-8をしていする)\n",
    "    encoding = response.info().get_content_charset(failobj='utf-8')\n",
    "\n",
    "    # デコードしてhtml形式のamazonホームページコードを取得\n",
    "    html = response.read().decode(encoding)\n",
    "    \"\"\"\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def scrape(html, base_url):\n",
    "    \"\"\"\n",
    "    \"\"全てのレビューページを取得する\"\"　ボタンのハイパーリンクを取得する\n",
    "    :param html: 特定の商品ページ(html形式ファイル)\n",
    "    :param base_url: amazonのホームページ(urlをstr型)\n",
    "    :return: 特定商品の全レビューを表示するページ(urlをstr型)\n",
    "    \"\"\"\n",
    "\n",
    "    doc = soup(html, \"lxml\")\n",
    "    if doc is not None:\n",
    "        a_tab = doc.find(\"a\", id=\"dp-summary-see-all-reviews\")\n",
    "        try:\n",
    "            if hasattr(a_tab, \"attrs\"):\n",
    "                # pathは相対パスで保存されているので絶対パスに変換する\n",
    "                abs_path = urljoin(base_url, quote(a_tab.attrs['href'].encode(\"utf-8\")))\n",
    "                return abs_path\n",
    "            else:\n",
    "                print(\"id=dp-summary-see-all-reviewsが無いようです_lxml\")\n",
    "        except\"It' gonna be passed because this page has not a a_tab(dp-summary-see-all-reviews)\":\n",
    "            return None\n",
    "    else:\n",
    "        print(\"docがNoneです\")\n",
    "\n",
    "\n",
    "def scrape_high_and_low(html, base_url):\n",
    "    \"\"\"\n",
    "    アマゾンにより特定商品につき高評価・低評価にリストアップされたレビューページへのハイパーリンクをリストで返す\n",
    "    :param html: \"全てのレビューページを取得する(1ページ目)\"のページをhtml形式\n",
    "    :param base_url: アマゾンのホームページ(urlをstr型)\n",
    "    :return:　高評価・低評価へのハイパーリンク(list型)\n",
    "    \"\"\"\n",
    "\n",
    "    high_and_low = []\n",
    "    doc = soup(html, \"lxml\")\n",
    "    if doc is not None:\n",
    "        a_tabs = doc.find_all(\"a\", class_=\"a-size-base a-link-normal see-all\")\n",
    "        for a_tab in a_tabs:\n",
    "\n",
    "            if hasattr(a_tab, \"attrs\"):\n",
    "                if \"高評価のレビュー\" in a_tab.text:\n",
    "                    # pathは相対パスで保存されているので絶対パスに変換する\n",
    "                    high_abs_path = urljoin(base_url, quote(a_tab.attrs['href'].encode(\"utf-8\")))\n",
    "                    high_and_low.append(high_abs_path)\n",
    "                elif \"低評価のレビュー\" in a_tab.text:\n",
    "                    # pathは相対パスで保存されているので絶対パスに変換する\n",
    "                    low_abs_path = urljoin(base_url, quote(a_tab.attrs['href'].encode(\"utf-8\")))\n",
    "                    high_and_low.append(low_abs_path)\n",
    "\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        return high_and_low\n",
    "\n",
    "    else:\n",
    "        print(\"docがNoneです\")\n",
    "\n",
    "\n",
    "def scrape_next_button(html, base_url):\n",
    "    \"\"\"\n",
    "    商品レビューページ下部にある\"次へ\"ボタンのリンク(２ページ目url)を取得する\n",
    "    :param html: 各商品のレビューページホーム(１ページ目)\n",
    "    :param base_url: amazonホームページurl\n",
    "    :return: 次のページurl(str型)\n",
    "    \"\"\"\n",
    "\n",
    "    doc = soup(html, \"lxml\")\n",
    "    div = doc.find(\"div\", id=\"cm_cr-pagination_bar\")\n",
    "\n",
    "    if div is not None:\n",
    "        # \"次へ\"ボタンのa_tabを取得\n",
    "        next_a_tab = 0\n",
    "        for a in div.find_all(\"a\"):\n",
    "            next_a_tab = a\n",
    "        abs_path = urljoin(base_url, quote(next_a_tab.attrs['href'].encode(\"utf-8\")))\n",
    "        print(abs_path)\n",
    "        return abs_path, div\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_all_goods_review_pages_url(html, base_rul, products_directory):\n",
    "    \"\"\"\n",
    "    商品レビューページを全て取得し各商品ディレクトリごとに保存する\n",
    "    :param html:  各商品のレビューページホーム(１ページ目)\n",
    "    :param base_rul: amazonホームページurl(str型)\n",
    "    :param products_directory:　商品ディレクトリ名(str型)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    count = 2\n",
    "\n",
    "    # 商品レビューディレクトリにレビューページをhtml形式で保存\n",
    "    with open(\"./reviews/review_{}/{}/{}\".format(GOODSNAME, products_directory,\n",
    "                                         str(0) + \"1\".zfill(3)) + \".html\", \"w\") as file:\n",
    "        file.write(html)\n",
    "\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        abs_path, div = scrape_next_button(html, base_rul)\n",
    "        if abs_path is not None and div is not None:\n",
    "            html = fetch(abs_path)\n",
    "            with open(\"./reviews/review_{}/{}/{}\".format(GOODSNAME, products_directory,\n",
    "                                                 str(0)+str(count).zfill(3)) + \".html\", \"w\") as file:\n",
    "                file.write(html)\n",
    "                count += 1\n",
    "                if div.find_all(\"li\", class_=\"a-disabled a-last\"):\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "    print(\"next\")\n",
    "\n",
    "\n",
    "def get_all_high_and_low_review(html, base_rul, products_directory, HIGH_OR_LOW_DIRECTORY):\n",
    "    \"\"\"\n",
    "    商品レビューページを高評価および低評価ごとに全て取得し各商品ディレクトリごとに保存する\n",
    "    :param html:  各商品のレビューページホーム(１ページ目)\n",
    "    :param base_rul: amazonホームページurl(str型)\n",
    "    :param products_directory:　商品ディレクトリ名(str型)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    count = 2\n",
    "\n",
    "    # 商品レビューディレクトリにレビューページをhtml形式で保存\n",
    "    with open(\"./reviews/review_{}/{}/{}/{}\".format(GOODSNAME, products_directory, HIGH_OR_LOW_DIRECTORY, str(0) + \"1\".zfill(3))\n",
    "              + \".html\", \"w\") as file:\n",
    "        file.write(html)\n",
    "\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        abs_path, div = scrape_next_button(html, base_rul)\n",
    "        if abs_path is not None and div is not None:\n",
    "            html = fetch(abs_path)\n",
    "            with open(\"./reviews/review_{}/{}/{}/{}\".format(GOODSNAME, products_directory, HIGH_OR_LOW_DIRECTORY, str(0)+str(count).zfill(3))\n",
    "                      + \".html\", \"w\") as file:\n",
    "                file.write(html)\n",
    "                count += 1\n",
    "                if div.find_all(\"li\", class_=\"a-disabled a-last\"):\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "    print(\"=====\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # 全てのメリーズ商品ページurlを取得する\n",
    "    urls = open(\"./text/all_goods_urls.txt\", \"r\").readlines()\n",
    "    count = 0\n",
    "    for url in urls:\n",
    "\n",
    "        # 商品ページ(html)を取得\n",
    "        html = fetch(url)\n",
    "        if html is None:\n",
    "            html = \"None\"\n",
    "        # 商品ページをhtml形式で保存\n",
    "        goods_label = url.split(\"&\")\n",
    "        with open(\"./goods_html/{}\".format(goods_label[-1])+\".html\", \"w\") as file:\n",
    "            file.write(html)\n",
    "\n",
    "        # \"\"全てのレビューページを取得する\"\"　ボタンのハイパーリンクを取得 (レビュー１ページ目)\n",
    "        url = scrape(html, BASEURL)\n",
    "        if url is not None:\n",
    "            # クローリング対象商品のurlを確認\n",
    "            print(\"[\"+str(count)+\"]\"+\" \"+\"url:\"+url)\n",
    "\n",
    "        # urlがbyte文字列の場合utf-8にdecodeしておく\n",
    "        if type(url) == bytes:\n",
    "            url = url.decode(\"utf-8\")\n",
    "\n",
    "        # ページレビューを保存するディレクトリを作成(レビューがある商品のみ)\n",
    "        if url is not None:\n",
    "            # 商品IDをurlから正規表現を利用し抜き出す\n",
    "            GOODSID = re.findall('B0[A-Z0-9]{8}', url)\n",
    "            os.makedirs(\"./reviews/review_{}/{}\".format(GOODSNAME, GOODSID[0]), exist_ok=True)\n",
    "\n",
    "            # \"\"全てのレビューページ\"\"のホーム(1ページ目)をhtml形式で取得\n",
    "            html_origin = fetch(url)\n",
    "\n",
    "            if html_origin is not None:\n",
    "\n",
    "                # アマゾンにより\"高評価\"および\"低評価\"にそれぞれリストアップされたページのハイパーリンクを取得(それぞれ1ページ目)\n",
    "                high_and_low_list = scrape_high_and_low(html_origin, BASEURL)\n",
    "\n",
    "                COUNT = 0\n",
    "                for url_high_and_low in high_and_low_list:\n",
    "                    DIRECTORY = 'HIGH--SCORED'\n",
    "                    if COUNT == 1:\n",
    "                        DIRECTORY = 'LOW--SCORED'\n",
    "                    print(DIRECTORY)\n",
    "                    if url_high_and_low is not None:\n",
    "\n",
    "                        # 商品IDをurlから正規表現を利用し抜き出す\n",
    "                        GOODSID = re.findall('B0[A-Z0-9]{8}', url_high_and_low)\n",
    "                        os.makedirs(\"./reviews/review_{}/{}/{}\".format(GOODSNAME, GOODSID[0], DIRECTORY), exist_ok=True)\n",
    "\n",
    "                        # \"\"全てのレビューページ\"\"のホーム(1ページ目)をhtml形式で取得\n",
    "                        html = fetch(url_high_and_low)\n",
    "\n",
    "                        if html is not None:\n",
    "\n",
    "                            # 商品に対応する全てのレビューを高評価および低評価ごとにクローリングする\n",
    "                            get_all_high_and_low_review(html, BASEURL, GOODSID[0], DIRECTORY)\n",
    "                            COUNT += 1\n",
    "\n",
    "                        else:\n",
    "                            print(\"この商品にはアマゾンによりリスト化された高評価・低評価のレビューページがありません\")\n",
    "                    else:\n",
    "                        print(\"変数のurl_high_and_lowがNoneです\")\n",
    "                #print(\"ALL REVIEW\")\n",
    "                # 高評価・低評価にかかわらず、商品に対応する全てのレビューをクローリングする\n",
    "                #get_all_goods_review_pages_url(html_origin, BASEURL, GOODSID[0])\n",
    "                count += 1\n",
    "                print(\"next\")\n",
    "            else:\n",
    "                print(\"この商品にはレビューページがありません\")\n",
    "        else:\n",
    "            print(\"変数のurlがNoneです\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
